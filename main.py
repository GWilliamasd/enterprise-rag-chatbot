import streamlit as st
import os
import json
import time
from datetime import datetime
from typing import List, Dict, Any
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# å¯¼å…¥ä¼ä¸šRAGç³»ç»Ÿï¼ˆå‡è®¾åœ¨rag_system.pyæ–‡ä»¶ä¸­ï¼‰
try:
    from rag_system import EnterpriseRAGSystem, DocumentProcessor
except ImportError:
    st.error("è¯·ç¡®ä¿ rag_system.py æ–‡ä»¶å­˜åœ¨å¹¶åŒ…å« EnterpriseRAGSystem ç±»")
    st.stop()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ğŸ¤– ä¼ä¸šçº§RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
        border-left: 4px solid #1f77b4;
    }
    .user-message {
        background-color: #f0f2f6;
    }
    .bot-message {
        background-color: #e8f4fd;
    }
    .source-doc {
        background-color: #f8f9fa;
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
        font-size: 0.9rem;
    }
    .metric-card {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    .upload-zone {
        border: 2px dashed #1f77b4;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
        background-color: #f8f9fa;
    }
</style>
""", unsafe_allow_html=True)


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€"""
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'documents_processed' not in st.session_state:
        st.session_state.documents_processed = 0
    if 'system_initialized' not in st.session_state:
        st.session_state.system_initialized = False
    if 'processing_logs' not in st.session_state:
        st.session_state.processing_logs = []


def load_config():
    """åŠ è½½RAGç³»ç»Ÿé…ç½®"""
    return {
        "llm_type": "ollama",  # ä½¿ç”¨Ollama
        "llm_config": {
            "model": "llama3.1:latest",
            "temperature": 0.1,
            "base_url": "http://localhost:11434"
        },
        "embedding_type": "huggingface",
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_k": 5
    }


def initialize_rag_system(sidebar_config=None):
    """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    if not st.session_state.system_initialized:
        with st.spinner("ğŸš€ æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ..."):
            try:
                config = load_config()
                
                # ä½¿ç”¨ä¾§è¾¹æ é…ç½®æ›´æ–°ç³»ç»Ÿé…ç½®
                if sidebar_config:
                    config["llm_config"]["model"] = sidebar_config["llm_model"]
                    config["llm_config"]["temperature"] = sidebar_config["temperature"]
                    config["retrieval_k"] = sidebar_config["retrieval_k"]
                    config["embedding_model"] = sidebar_config["embedding_model"]
                    
                    # æ ¹æ®LLMæ¨¡å‹è®¾ç½®ç±»å‹
                    if sidebar_config["llm_model"] == "gpt-4o-mini":
                        config["llm_type"] = "openai"
                        config["llm_config"]["api_key"] = os.getenv("OPENAI_API_KEY")
                    else:
                        config["llm_type"] = "ollama"
                    
                    # æ ¹æ®embeddingæ¨¡å‹è®¾ç½®ç±»å‹
                    if sidebar_config["embedding_model"].startswith("sentence-transformers"):
                        config["embedding_type"] = "huggingface"
                    else:
                        config["embedding_type"] = "ollama"
                
                st.session_state.rag_system = EnterpriseRAGSystem(config)
                st.session_state.system_initialized = True
                st.session_state.current_config = sidebar_config  # ä¿å­˜å½“å‰é…ç½®
                st.success("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
                return True
            except Exception as e:
                st.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
    return True


def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ """
    st.sidebar.title("ğŸ› ï¸ ç³»ç»Ÿæ§åˆ¶å°")

    # ç³»ç»ŸçŠ¶æ€
    st.sidebar.subheader("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    if st.session_state.system_initialized:
        st.sidebar.success("ğŸŸ¢ ç³»ç»Ÿå·²å°±ç»ª")
    else:
        st.sidebar.error("ğŸ”´ ç³»ç»Ÿæœªåˆå§‹åŒ–")

    # ç»Ÿè®¡ä¿¡æ¯
    col1, col2 = st.sidebar.columns(2)
    with col1:
        st.metric("ğŸ“„ å·²å¤„ç†æ–‡æ¡£", st.session_state.documents_processed)
    with col2:
        st.metric("ğŸ’¬ å¯¹è¯æ¬¡æ•°", len(st.session_state.chat_history))

    # é…ç½®é€‰é¡¹
    st.sidebar.subheader("âš™ï¸ ç³»ç»Ÿé…ç½®")

    # LLMæ¨¡å‹é€‰æ‹©
    available_models = ["llama3.1:latest", "qwen3:4b"]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰OpenAI APIå¯†é’¥
    if os.getenv("OPENAI_API_KEY"):
        available_models.append("gpt-4o-mini")
    
    llm_model = st.sidebar.selectbox(
        "é€‰æ‹©è¯­è¨€æ¨¡å‹",
        available_models,
        index=0,
        help="ç”¨äºç”Ÿæˆå›ç­”çš„å¤§è¯­è¨€æ¨¡å‹"
    )
    
    # Embeddingæ¨¡å‹é€‰æ‹©
    embedding_model = st.sidebar.selectbox(
        "é€‰æ‹©åµŒå…¥æ¨¡å‹",
        ["sentence-transformers/all-MiniLM-L6-v2", "nomic-embed-text"],
        index=0,
        help="ç”¨äºæ–‡æœ¬å‘é‡åŒ–çš„æ¨¡å‹"
    )

    # æ£€ç´¢å‚æ•°
    retrieval_k = st.sidebar.slider("æ£€ç´¢æ–‡æ¡£æ•°é‡", 1, 10, 5)
    temperature = st.sidebar.slider("æ¨¡å‹æ¸©åº¦", 0.0, 1.0, 0.1, 0.1)

    # é«˜çº§é€‰é¡¹
    with st.sidebar.expander("ğŸ”§ é«˜çº§é€‰é¡¹"):
        chunk_size = st.sidebar.slider("æ–‡æ¡£å—å¤§å°", 500, 2000, 1000, 100)
        chunk_overlap = st.sidebar.slider("æ–‡æ¡£å—é‡å ", 50, 500, 200, 50)
        enable_cache = st.sidebar.checkbox("å¯ç”¨ç¼“å­˜", True)

    # ç³»ç»Ÿé‡ç½®
    if st.sidebar.button("ğŸ”„ é‡ç½®ç³»ç»Ÿ", type="secondary"):
        st.session_state.clear()
        st.rerun()

    return {
        "llm_model": llm_model,
        "embedding_model": embedding_model,
        "retrieval_k": retrieval_k,
        "temperature": temperature,
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "enable_cache": enable_cache
    }


def render_document_upload():
    """æ¸²æŸ“æ–‡æ¡£ä¸Šä¼ ç•Œé¢"""
    st.subheader("ğŸ“ æ–‡æ¡£ç®¡ç†")

    # æ–‡æ¡£ä¸Šä¼ åŒºåŸŸ
    st.markdown('<div class="upload-zone">', unsafe_allow_html=True)
    st.markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
    st.markdown("æ”¯æŒæ ¼å¼ï¼šPDFã€Wordã€Excelã€TXT")

    uploaded_files = st.file_uploader(
        "é€‰æ‹©æ–‡ä»¶",
        type=['pdf', 'docx', 'xlsx', 'txt'],
        accept_multiple_files=True,
        key="doc_uploader"
    )
    st.markdown('</div>', unsafe_allow_html=True)

    # é›†åˆåç§°è¾“å…¥
    collection_name = st.text_input(
        "çŸ¥è¯†åº“åç§°",
        value="default",
        help="ä¸ºæ‚¨çš„æ–‡æ¡£é›†åˆå‘½å"
    )

    # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    if uploaded_files and st.button("ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£", type="primary"):
        process_uploaded_documents(uploaded_files, collection_name)

    # æ˜¾ç¤ºå·²ä¸Šä¼ æ–‡æ¡£åˆ—è¡¨
    render_document_list()


def process_uploaded_documents(uploaded_files: List, collection_name: str):
    """å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£"""
    if not st.session_state.system_initialized:
        st.error("âŒ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ")
        return

    # åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    temp_dir = "temp_uploads"
    os.makedirs(temp_dir, exist_ok=True)

    file_paths = []

    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    for uploaded_file in uploaded_files:
        file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        file_paths.append(file_path)

    # æ˜¾ç¤ºå¤„ç†è¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        # è°ƒç”¨RAGç³»ç»Ÿå¤„ç†æ–‡æ¡£
        status_text.text("ğŸ“ æ­£åœ¨å¤„ç†æ–‡æ¡£...")
        result = st.session_state.rag_system.ingest_documents(file_paths, collection_name)

        progress_bar.progress(100)

        if result['status'] == 'success':
            st.success(f"âœ… æˆåŠŸå¤„ç† {result['documents_processed']} ä¸ªæ–‡æ¡£å—")
            st.session_state.documents_processed += len(uploaded_files)

            # è®°å½•å¤„ç†æ—¥å¿—
            log_entry = {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "action": "æ–‡æ¡£ä¸Šä¼ ",
                "files": [f.name for f in uploaded_files],
                "collection": collection_name,
                "status": "æˆåŠŸ"
            }
            st.session_state.processing_logs.append(log_entry)

        else:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {result['message']}")

    except Exception as e:
        st.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for file_path in file_paths:
            try:
                os.remove(file_path)
            except:
                pass

        progress_bar.empty()
        status_text.empty()


def render_document_list():
    """æ˜¾ç¤ºå·²å¤„ç†æ–‡æ¡£åˆ—è¡¨"""
    if st.session_state.processing_logs:
        st.subheader("ğŸ“‹ å¤„ç†å†å²")

        # è½¬æ¢ä¸ºDataFrameæ˜¾ç¤º
        df = pd.DataFrame(st.session_state.processing_logs)

        # åªæ˜¾ç¤ºæœ€è¿‘çš„10æ¡è®°å½•
        recent_logs = df.tail(10).iloc[::-1]  # é€†åºæ˜¾ç¤º

        for _, log in recent_logs.iterrows():
            with st.expander(f"ğŸ“„ {log['timestamp']} - {log['action']}"):
                st.write(f"**æ–‡ä»¶:** {', '.join(log['files'])}")
                st.write(f"**é›†åˆ:** {log['collection']}")
                st.write(f"**çŠ¶æ€:** {log['status']}")


def render_chat_interface():
    """æ¸²æŸ“èŠå¤©ç•Œé¢"""
    st.subheader("ğŸ’¬ æ™ºèƒ½é—®ç­”")

    if not st.session_state.system_initialized:
        st.warning("âš ï¸ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿæ‰èƒ½å¼€å§‹å¯¹è¯")
        return

    # æ˜¾ç¤ºèŠå¤©å†å²
    chat_container = st.container()

    with chat_container:
        for i, (question, answer, sources, response_time) in enumerate(st.session_state.chat_history):
            # ç”¨æˆ·æ¶ˆæ¯
            st.markdown(f'''
            <div class="chat-message user-message">
                <strong>ğŸ™‹ æ‚¨:</strong> {question}
            </div>
            ''', unsafe_allow_html=True)

            # AIå›ç­”
            st.markdown(f'''
            <div class="chat-message bot-message">
                <strong>ğŸ¤– AIåŠ©æ‰‹:</strong> {answer}
                <br><small>â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’</small>
            </div>
            ''', unsafe_allow_html=True)

            # æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            if sources:
                with st.expander(f"ğŸ“š æŸ¥çœ‹æ¥æºæ–‡æ¡£ ({len(sources)}ä¸ª)"):
                    for j, source in enumerate(sources):
                        st.markdown(f'''
                        <div class="source-doc">
                            <strong>æ¥æº {j + 1}:</strong> {source['metadata'].get('source', 'æœªçŸ¥æ¥æº')}<br>
                            <em>{source['content'][:200]}...</em>
                        </div>
                        ''', unsafe_allow_html=True)

    # èŠå¤©è¾“å…¥æ¡†
    st.markdown("---")

    # ä½¿ç”¨formæ¥å¤„ç†è¾“å…¥
    with st.form(key="chat_form", clear_on_submit=True):
        col1, col2 = st.columns([4, 1])

        with col1:
            user_input = st.text_input(
                "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šå…¬å¸çš„ä¼‘å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
                key="user_question"
            )

        with col2:
            submit_button = st.form_submit_button("å‘é€ ğŸ“¤", type="primary")

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if submit_button and user_input:
        handle_user_query(user_input)


def handle_user_query(question: str):
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
    if not st.session_state.rag_system:
        st.error("âŒ ç³»ç»Ÿæœªåˆå§‹åŒ–")
        return

    with st.spinner("ğŸ¤” AIæ­£åœ¨æ€è€ƒä¸­..."):
        try:
            # è°ƒç”¨RAGç³»ç»ŸæŸ¥è¯¢
            start_time = time.time()
            response = st.session_state.rag_system.query(question, user_id="streamlit_user")
            end_time = time.time()

            # æ·»åŠ åˆ°èŠå¤©å†å²
            st.session_state.chat_history.append((
                question,
                response['answer'],
                response.get('source_documents', []),
                response.get('response_time', end_time - start_time)
            ))

            # åˆ·æ–°é¡µé¢æ˜¾ç¤ºæ–°æ¶ˆæ¯
            st.rerun()

        except Exception as e:
            st.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")


def render_analytics_dashboard():
    """æ¸²æŸ“åˆ†æä»ªè¡¨æ¿"""
    st.subheader("ğŸ“Š ç³»ç»Ÿåˆ†æ")

    if not st.session_state.system_initialized:
        st.info("â„¹ï¸ åˆå§‹åŒ–ç³»ç»Ÿåå¯æŸ¥çœ‹åˆ†ææ•°æ®")
        return

    # è·å–ç³»ç»Ÿç»Ÿè®¡
    try:
        stats = st.session_state.rag_system.get_system_stats()

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown('''
            <div class="metric-card">
                <h3>ğŸ“„</h3>
                <h2>{}</h2>
                <p>å‘é‡åº“å¤§å°</p>
            </div>
            '''.format(stats.get('vector_store_size', 0)), unsafe_allow_html=True)

        with col2:
            st.markdown('''
            <div class="metric-card">
                <h3>ğŸ’¬</h3>
                <h2>{}</h2>
                <p>æ€»æŸ¥è¯¢æ¬¡æ•°</p>
            </div>
            '''.format(stats.get('total_queries', 0)), unsafe_allow_html=True)

        with col3:
            st.markdown('''
            <div class="metric-card">
                <h3>âš¡</h3>
                <h2>{:.2f}s</h2>
                <p>å¹³å‡å“åº”æ—¶é—´</p>
            </div>
            '''.format(stats.get('avg_response_time', 0)), unsafe_allow_html=True)

        with col4:
            st.markdown('''
            <div class="metric-card">
                <h3>ğŸ¯</h3>
                <h2>{}</h2>
                <p>ä»Šæ—¥æŸ¥è¯¢</p>
            </div>
            '''.format(len(st.session_state.chat_history)), unsafe_allow_html=True)

        # å“åº”æ—¶é—´è¶‹åŠ¿å›¾
        if st.session_state.chat_history:
            st.subheader("ğŸ“ˆ å“åº”æ—¶é—´è¶‹åŠ¿")

            response_times = [chat[3] for chat in st.session_state.chat_history]
            query_numbers = list(range(1, len(response_times) + 1))

            fig = px.line(
                x=query_numbers,
                y=response_times,
                title="æŸ¥è¯¢å“åº”æ—¶é—´å˜åŒ–",
                labels={'x': 'æŸ¥è¯¢æ¬¡æ•°', 'y': 'å“åº”æ—¶é—´ (ç§’)'}
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

        # æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ
        if st.session_state.chat_history:
            st.subheader("ğŸ“Š æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ")

            query_lengths = [len(chat[0]) for chat in st.session_state.chat_history]

            fig = px.histogram(
                x=query_lengths,
                nbins=10,
                title="ç”¨æˆ·æŸ¥è¯¢é•¿åº¦åˆ†å¸ƒ",
                labels={'x': 'æŸ¥è¯¢é•¿åº¦ (å­—ç¬¦)', 'y': 'é¢‘æ¬¡'}
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")


def render_system_logs():
    """æ¸²æŸ“ç³»ç»Ÿæ—¥å¿—"""
    st.subheader("ğŸ“‹ ç³»ç»Ÿæ—¥å¿—")

    # æŸ¥çœ‹å¤„ç†æ—¥å¿—
    if st.session_state.processing_logs:
        st.write("**æ–‡æ¡£å¤„ç†æ—¥å¿—:**")
        df = pd.DataFrame(st.session_state.processing_logs)
        st.dataframe(df, use_container_width=True)

    # æŸ¥çœ‹æŸ¥è¯¢æ—¥å¿—
    if st.session_state.chat_history:
        st.write("**æŸ¥è¯¢å†å²:**")
        chat_df = pd.DataFrame([
            {
                "æ—¶é—´": datetime.now().strftime("%H:%M:%S"),
                "é—®é¢˜": chat[0][:50] + "..." if len(chat[0]) > 50 else chat[0],
                "å“åº”æ—¶é—´": f"{chat[3]:.2f}s",
                "æ¥æºæ•°é‡": len(chat[2])
            }
            for chat in st.session_state.chat_history
        ])
        st.dataframe(chat_df, use_container_width=True)

    # å¯¼å‡ºæ—¥å¿—
    if st.button("ğŸ“¥ å¯¼å‡ºæ—¥å¿—"):
        export_logs()


def export_logs():
    """å¯¼å‡ºç³»ç»Ÿæ—¥å¿—"""
    try:
        log_data = {
            "processing_logs": st.session_state.processing_logs,
            "chat_history": [
                {
                    "question": chat[0],
                    "answer": chat[1],
                    "response_time": chat[3],
                    "source_count": len(chat[2])
                }
                for chat in st.session_state.chat_history
            ],
            "export_time": datetime.now().isoformat()
        }

        log_json = json.dumps(log_data, ensure_ascii=False, indent=2)

        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ—¥å¿—æ–‡ä»¶",
            data=log_json,
            file_name=f"rag_system_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )

    except Exception as e:
        st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()

    # ä¸»æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸ¤– ä¼ä¸šçº§RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ</h1>', unsafe_allow_html=True)

    # æ¸²æŸ“ä¾§è¾¹æ 
    sidebar_config = render_sidebar()

    # ä¸»è¦å†…å®¹åŒºåŸŸ
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ æ™ºèƒ½é—®ç­”", "ğŸ“ æ–‡æ¡£ç®¡ç†", "ğŸ“Š æ•°æ®åˆ†æ", "ğŸ“‹ ç³»ç»Ÿæ—¥å¿—"])

    # åˆå§‹åŒ–ç³»ç»Ÿ
    if not st.session_state.system_initialized:
        if st.button("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ", type="primary", key="init_system"):
            initialize_rag_system(sidebar_config)
    else:
        # ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
        if hasattr(st.session_state, 'current_config'):
            current_model = st.session_state.current_config.get('llm_model', 'æœªçŸ¥')
            st.sidebar.success(f"ğŸ¤– å½“å‰æ¨¡å‹: {current_model}")
        
        # æ£€æŸ¥é…ç½®æ˜¯å¦æœ‰å˜åŒ–
        if st.session_state.get('current_config') != sidebar_config:
            if st.sidebar.button("ğŸ”„ åº”ç”¨æ–°é…ç½®", type="primary"):
                with st.spinner("æ­£åœ¨æ›´æ–°ç³»ç»Ÿé…ç½®..."):
                    try:
                        # å…ˆæ¸…ç©ºç¼“å­˜ä»¥ç¡®ä¿ä½¿ç”¨æ–°æ¨¡å‹
                        st.session_state.rag_system.clear_cache()
                        
                        # æ›´æ–°é…ç½®
                        config = load_config()
                        config["llm_config"]["model"] = sidebar_config["llm_model"]
                        config["llm_config"]["temperature"] = sidebar_config["temperature"]
                        config["retrieval_k"] = sidebar_config["retrieval_k"]
                        config["embedding_model"] = sidebar_config["embedding_model"]
                        
                        # æ ¹æ®LLMæ¨¡å‹è®¾ç½®ç±»å‹
                        if sidebar_config["llm_model"] == "gpt-4o-mini":
                            config["llm_type"] = "openai"
                            config["llm_config"]["api_key"] = os.getenv("OPENAI_API_KEY")
                        else:
                            config["llm_type"] = "ollama"
                        
                        # æ ¹æ®embeddingæ¨¡å‹è®¾ç½®ç±»å‹
                        if sidebar_config["embedding_model"].startswith("sentence-transformers"):
                            config["embedding_type"] = "huggingface"
                        else:
                            config["embedding_type"] = "ollama"
                        
                        # æ›´æ–°RAGç³»ç»Ÿé…ç½®
                        success = st.session_state.rag_system.update_config(config)
                        if success:
                            st.session_state.current_config = sidebar_config
                            st.success(f"âœ… å·²åˆ‡æ¢åˆ°: {sidebar_config['llm_model']}")
                        else:
                            st.error("âŒ é…ç½®æ›´æ–°å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
                    except Exception as e:
                        st.error(f"âŒ é…ç½®æ›´æ–°å¤±è´¥: {str(e)}")
            else:
                st.sidebar.warning("âš ï¸ é…ç½®å·²å˜æ›´ï¼Œç‚¹å‡»'åº”ç”¨æ–°é…ç½®'ç”Ÿæ•ˆ")

    with tab1:
        render_chat_interface()

    with tab2:
        render_document_upload()

    with tab3:
        render_analytics_dashboard()

    with tab4:
        render_system_logs()

    # é¡µè„š
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "ğŸ”§ ä¼ä¸šçº§RAGç³»ç»Ÿ | åŸºäºLangChain + Streamlitæ„å»º"
        "</div>",
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()

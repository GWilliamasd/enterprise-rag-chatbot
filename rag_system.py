"""
ä¼ä¸šçº§RAGç³»ç»Ÿå®ç°
åŸºäºlangchainæ„å»ºçš„å®Œæ•´RAGè§£å†³æ–¹æ¡ˆ
"""

import os
import json
import logging
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
import sqlite3

from curl_cffi import Response
from dotenv import load_dotenv
load_dotenv()
# langchain imports
from langchain_ollama import OllamaLLM
from langchain_ollama import ChatOllama
from langchain_ollama import OllamaEmbeddings
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredExcelLoader, CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

class SmartCacheSystem:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_file: str = "rag_cache.db"):
        self.cache_file = cache_file
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“"""
        conn = sqlite3.connect(self.cache_file)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS query_cache (
                query_hash TEXT PRIMARY KEY,
                query_text TEXT,
                response TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
    
    def get_cached_response(self, query: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        conn = sqlite3.connect(self.cache_file)
        cursor = conn.cursor()
        cursor.execute(
            'SELECT response FROM query_cache WHERE query_hash = ?',
            (query_hash,)
        )
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result else None
    
    def cache_response(self, query: str, response: str):
        """ç¼“å­˜å“åº”"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        conn = sqlite3.connect(self.cache_file)
        cursor = conn.cursor()
        cursor.execute(
            'INSERT OR REPLACE INTO query_cache (query_hash, query_text, response) VALUES (?, ?, ?)',
            (query_hash, query, response)
        )
        conn.commit()
        conn.close()

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.loaders = {
            ".pdf": PyPDFLoader,
            ".txt": TextLoader,
            ".docx": UnstructuredWordDocumentLoader,
            ".xlsx": UnstructuredExcelLoader,
            ".csv": CSVLoader
        }
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
    
    def load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£"""
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext not in self.loaders:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        try:
            loader_class = self.loaders[file_ext]
            loader = loader_class(file_path)
            documents = loader.load()
            
            # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°å…ƒæ•°æ®
            for doc in documents:
                doc.metadata['source'] = file_path
                doc.metadata['file_type'] = file_ext
                doc.metadata['processed_time'] = datetime.now().isoformat()
            
            return documents
        
        except Exception as e:
            logging.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {str(e)}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            return self.text_splitter.split_documents(documents)
        except Exception as e:
            logging.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
            return documents

class EnterpriseRAGSystem:
    """ä¼ä¸šçº§RAGç³»ç»Ÿ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = self._setup_logging()
        self.cache_system = SmartCacheSystem()
        self.doc_processor = DocumentProcessor()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = None
        self.embeddings = None
        self.vector_store = None
        self.retriever = None
        
        self._initialize_components()
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rag_system.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def _initialize_components(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        try:
            # åˆå§‹åŒ–LLM
            self._initialize_llm()
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self._initialize_embeddings()
            
            self.logger.info("RAGç³»ç»Ÿç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        llm_config = self.config.get("llm_config", {})
        
        if self.config.get("llm_type") == "ollama":
            self.llm = OllamaLLM(
                model=llm_config.get("model", "llama3.1:latest"),
                temperature=llm_config.get("temperature", 0.1),
                base_url=llm_config.get("base_url", "http://localhost:11434")
            )
        elif self.config.get("llm_type") == "openai":
            self.llm = ChatOpenAI(
                model=llm_config.get("model", "gpt-4o-mini"),
                temperature=llm_config.get("temperature", 0.1),
                api_key=llm_config.get("api_key")
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMç±»å‹: {self.config.get('llm_type')}")
    
    def _initialize_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        if self.config.get("embedding_type") == "huggingface":
            model_name = self.config.get("embedding_model", "sentence-transformers/all-MiniLM-L6-v2")
            self.embeddings = HuggingFaceEmbeddings(model_name=model_name)
        elif self.config.get("embedding_type") == "ollama":
            self.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        else:
            # é»˜è®¤ä½¿ç”¨HuggingFace
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
    
    def ingest_documents(self, file_paths: List[str], collection_name: str = "default") -> Dict[str, Any]:
        """æ‰¹é‡å¯¼å…¥æ–‡æ¡£"""
        self.logger.info(f"å¼€å§‹å¯¼å…¥{len(file_paths)}ä¸ªæ–‡æ¡£åˆ°é›†åˆ: {collection_name}")
        
        try:
            # åŠ è½½æ‰€æœ‰æ–‡æ¡£
            all_docs = []
            processed_files = []
            
            for file_path in file_paths:
                if os.path.exists(file_path):
                    docs = self.doc_processor.load_document(file_path)
                    if docs:
                        all_docs.extend(docs)
                        processed_files.append(file_path)
                        self.logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}")
                    else:
                        self.logger.warning(f"æ–‡æ¡£åŠ è½½ä¸ºç©º: {file_path}")
                else:
                    self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            if not all_docs:
                return {"status": "error", "message": "æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£"}
            
            # åˆ†å‰²æ–‡æ¡£
            split_docs = self.doc_processor.split_documents(all_docs)
            self.logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±{len(split_docs)}ä¸ªå—")
            
            # åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨
            vector_db_path = f"./vectordb/{collection_name}"
            os.makedirs(vector_db_path, exist_ok=True)
            
            if self.vector_store is None:
                # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                self.vector_store = Chroma.from_documents(
                    documents=split_docs,
                    embedding=self.embeddings,
                    persist_directory=vector_db_path,
                    collection_name=collection_name
                )
            else:
                # æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                self.vector_store.add_documents(split_docs)
            
            # æŒä¹…åŒ–å­˜å‚¨
            if hasattr(self.vector_store, 'persist'):
                self.vector_store.persist()
            
            # åˆ›å»ºæ£€ç´¢å™¨
            self.retriever = self.vector_store.as_retriever(
                search_kwargs={"k": self.config.get("retrieval_k", 5)}
            )
            
            self.logger.info(f"æˆåŠŸå¯¼å…¥{len(split_docs)}ä¸ªæ–‡æ¡£å—")
            
            return {
                "status": "success",
                "documents_processed": len(split_docs),
                "files_processed": processed_files,
                "collection_name": collection_name
            }
            
        except Exception as e:
            error_msg = f"æ–‡æ¡£å¯¼å…¥å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            return {"status": "error", "message": error_msg}
    
    def query(self, question: str, user_id: str = None) -> Dict[str, Any]:
        """æŸ¥è¯¢RAGç³»ç»Ÿ"""
        self.logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {question}")
        
        try:
            start_time = datetime.now()
            
            # æ£€æŸ¥ç¼“å­˜ï¼ˆä½†ä¸ºäº†æµ‹è¯•æ¨¡å‹åˆ‡æ¢ï¼Œæš‚æ—¶ç¦ç”¨ï¼‰
            # cached_response = self.cache_system.get_cached_response(question)
            # if cached_response:
            #     self.logger.info("ä»ç¼“å­˜è¿”å›å“åº”")
            #     return {
            #         "answer": cached_response,
            #         "source_documents": [],
            #         "response_time": 0.1,
            #         "from_cache": True
            #     }
            
            # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å°±ç»ª
            if self.retriever is None:
                return {
                    "answer": "ç³»ç»Ÿå°šæœªåŠ è½½ä»»ä½•æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ã€‚",
                    "source_documents": [],
                    "response_time": 0,
                    "error": "no_documents"
                }
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            relevant_docs = self.retriever.get_relevant_documents(question)
            
            if not relevant_docs:
                return {
                    "answer": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "source_documents": [],
                    "response_time": (datetime.now() - start_time).total_seconds()
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(relevant_docs)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = self._generate_answer(question, context)
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = (datetime.now() - start_time).total_seconds()
            
            # ç¼“å­˜å“åº”
            self.cache_system.cache_response(question, answer)
            
            # è®°å½•æŸ¥è¯¢æ—¥å¿—
            self._log_query(user_id, question, answer, response_time)
            
            # æ ¼å¼åŒ–æºæ–‡æ¡£
            formatted_sources = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in relevant_docs
            ]
            
            return {
                "answer": answer,
                "source_documents": formatted_sources,
                "response_time": response_time,
                "from_cache": False
            }
            
        except Exception as e:
            error_msg = f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            return {
                "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "source_documents": [],
                "response_time": (datetime.now() - start_time).total_seconds(),
                "error": str(e)
            }
    
    def _build_context(self, documents: List[Document]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            # æå–æ–‡ä»¶å
            if "/" in source or "\\" in source:
                source = os.path.basename(source)
            
            context_part = f"[æ–‡æ¡£{i}] æ¥æº: {source}\n{doc.page_content}\n"
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def _generate_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä¿æŒå›ç­”çš„ä¸“ä¸šæ€§å’Œå‹å¥½æ€§
4. å¯ä»¥é€‚å½“å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
5. å›ç­”è¦æ¡ç†æ¸…æ¥šï¼Œæ˜“äºç†è§£

å›ç­”ï¼š
"""
        
        try:
            prompt = prompt_template.format(context=context, question=question)
            
            # æ ¹æ®LLMç±»å‹è°ƒç”¨ä¸åŒæ–¹æ³•
            if self.config.get("llm_type") == "openai":
                response = self.llm.invoke(prompt)
                return response.content.strip()
            else:
                # Ollama LLM
                response = self.llm(prompt)
                return response.strip()
        
        except Exception as e:
            self.logger.error(f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def _log_query(self, user_id: str, question: str, answer: str, response_time: float):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—"""
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "user_id": user_id or "anonymous",
                "question": question,
                "answer": answer[:200] + "..." if len(answer) > 200 else answer,
                "response_time": response_time
            }
            
            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open("query_logs.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
                
        except Exception as e:
            self.logger.error(f"æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "vector_store_size": 0,
            "total_queries": 0,
            "avg_response_time": 0,
            "cache_size": 0
        }
        
        try:
            # å‘é‡å­˜å‚¨å¤§å°
            if self.vector_store:
                try:
                    if hasattr(self.vector_store, '_collection'):
                        stats["vector_store_size"] = self.vector_store._collection.count()
                    else:
                        stats["vector_store_size"] = "æœªçŸ¥"
                except:
                    stats["vector_store_size"] = "æœªçŸ¥"
            
            # æŸ¥è¯¢ç»Ÿè®¡
            try:
                if os.path.exists("query_logs.jsonl"):
                    with open("query_logs.jsonl", "r", encoding="utf-8") as f:
                        logs = [json.loads(line) for line in f if line.strip()]
                        stats["total_queries"] = len(logs)
                        
                        if logs:
                            response_times = [log.get("response_time", 0) for log in logs]
                            stats["avg_response_time"] = sum(response_times) / len(response_times)
            except Exception as e:
                self.logger.warning(f"è¯»å–æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {str(e)}")
            
            # ç¼“å­˜ç»Ÿè®¡
            try:
                conn = sqlite3.connect(self.cache_system.cache_file)
                cursor = conn.cursor()
                cursor.execute('SELECT COUNT(*) FROM query_cache')
                stats["cache_size"] = cursor.fetchone()[0]
                conn.close()
            except Exception as e:
                self.logger.warning(f"è¯»å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        except Exception as e:
            self.logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        return stats
    
    def load_existing_collection(self, collection_name: str = "default") -> bool:
        """åŠ è½½å·²å­˜åœ¨çš„æ–‡æ¡£é›†åˆ"""
        try:
            vector_db_path = f"./vectordb/{collection_name}"
            
            if os.path.exists(vector_db_path):
                self.vector_store = Chroma(
                    persist_directory=vector_db_path,
                    embedding_function=self.embeddings,
                    collection_name=collection_name
                )
                
                self.retriever = self.vector_store.as_retriever(
                    search_kwargs={"k": self.config.get("retrieval_k", 5)}
                )
                
                self.logger.info(f"æˆåŠŸåŠ è½½å·²å­˜åœ¨çš„é›†åˆ: {collection_name}")
                return True
            else:
                self.logger.info(f"é›†åˆä¸å­˜åœ¨: {collection_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"åŠ è½½é›†åˆå¤±è´¥: {str(e)}")
            return False
    
    def list_collections(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£é›†åˆ"""
        try:
            vectordb_dir = "./vectordb"
            if os.path.exists(vectordb_dir):
                collections = [
                    name for name in os.listdir(vectordb_dir)
                    if os.path.isdir(os.path.join(vectordb_dir, name))
                ]
                return collections
            return []
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºé›†åˆå¤±è´¥: {str(e)}")
            return []
    
    def delete_collection(self, collection_name: str) -> bool:
        """åˆ é™¤æ–‡æ¡£é›†åˆ"""
        try:
            import shutil
            vector_db_path = f"./vectordb/{collection_name}"
            
            if os.path.exists(vector_db_path):
                shutil.rmtree(vector_db_path)
                self.logger.info(f"æˆåŠŸåˆ é™¤é›†åˆ: {collection_name}")
                
                # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰é›†åˆï¼Œé‡ç½®ç³»ç»ŸçŠ¶æ€
                if self.vector_store and hasattr(self.vector_store, '_persist_directory'):
                    if self.vector_store._persist_directory == vector_db_path:
                        self.vector_store = None
                        self.retriever = None
                
                return True
            else:
                self.logger.warning(f"é›†åˆä¸å­˜åœ¨: {collection_name}")
                return False
                
        except Exception as e:
            self.logger.error(f"åˆ é™¤é›†åˆå¤±è´¥: {str(e)}")
            return False
    
    def update_config(self, new_config: Dict[str, Any]):
        """æ›´æ–°ç³»ç»Ÿé…ç½®"""
        try:
            self.config.update(new_config)
            
            # é‡æ–°åˆå§‹åŒ–ç›¸å…³ç»„ä»¶
            if "llm_config" in new_config or "llm_type" in new_config:
                self._initialize_llm()
            
            if "embedding_model" in new_config or "embedding_type" in new_config:
                self._initialize_embeddings()
                
            if "retrieval_k" in new_config and self.retriever:
                self.retriever = self.vector_store.as_retriever(
                    search_kwargs={"k": new_config["retrieval_k"]}
                )
            
            self.logger.info("é…ç½®æ›´æ–°æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}")
            return False
    
    def health_check(self) -> Dict[str, Any]:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        health_status = {
            "status": "healthy",
            "components": {},
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            # æ£€æŸ¥LLM
            health_status["components"]["llm"] = {
                "status": "ok" if self.llm else "error",
                "type": self.config.get("llm_type", "unknown")
            }
            
            # æ£€æŸ¥åµŒå…¥æ¨¡å‹
            health_status["components"]["embeddings"] = {
                "status": "ok" if self.embeddings else "error",
                "type": self.config.get("embedding_type", "unknown")
            }
            
            # æ£€æŸ¥å‘é‡å­˜å‚¨
            health_status["components"]["vector_store"] = {
                "status": "ok" if self.vector_store else "not_loaded",
                "has_documents": bool(self.retriever)
            }
            
            # æ£€æŸ¥ç¼“å­˜ç³»ç»Ÿ
            try:
                conn = sqlite3.connect(self.cache_system.cache_file)
                conn.close()
                health_status["components"]["cache"] = {"status": "ok"}
            except:
                health_status["components"]["cache"] = {"status": "error"}
            
            # æ£€æŸ¥æ—¥å¿—ç³»ç»Ÿ
            health_status["components"]["logging"] = {
                "status": "ok" if os.path.exists("rag_system.log") else "warning"
            }
            
            # æ€»ä½“çŠ¶æ€è¯„ä¼°
            component_statuses = [comp["status"] for comp in health_status["components"].values()]
            if "error" in component_statuses:
                health_status["status"] = "degraded"
            elif "warning" in component_statuses or "not_loaded" in component_statuses:
                health_status["status"] = "warning"
            
        except Exception as e:
            health_status["status"] = "error"
            health_status["error"] = str(e)
        
        return health_status
    
    def clear_cache(self) -> bool:
        """æ¸…ç†ç¼“å­˜"""
        try:
            conn = sqlite3.connect(self.cache_system.cache_file)
            cursor = conn.cursor()
            cursor.execute('DELETE FROM query_cache')
            conn.commit()
            conn.close()
            
            self.logger.info("ç¼“å­˜æ¸…ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
            return False

# å·¥å…·å‡½æ•°
def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        "llm_type": "ollama",
        "llm_config": {
            "model": "llama3.1:latest",
            "temperature": 0.1
        },
        "embedding_type": "huggingface",
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_k": 5,
        "chunk_size": 1000,
        "chunk_overlap": 200
    }

def test_system():
    """æµ‹è¯•ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹RAGç³»ç»Ÿæµ‹è¯•...")
    
    # åˆ›å»ºé…ç½®
    config = create_default_config()
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        print("ğŸ“Š åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        rag_system = EnterpriseRAGSystem(config)
        
        # å¥åº·æ£€æŸ¥
        print("ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥...")
        health = rag_system.health_check()
        print(f"ç³»ç»ŸçŠ¶æ€: {health['status']}")
        
        # æµ‹è¯•æŸ¥è¯¢ï¼ˆæ— æ–‡æ¡£ï¼‰
        print("â“ æµ‹è¯•ç©ºæŸ¥è¯¢...")
        response = rag_system.query("æµ‹è¯•é—®é¢˜")
        print(f"å“åº”: {response['answer'][:100]}...")
        
        print("âœ… ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    test_system()